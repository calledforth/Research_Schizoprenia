{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Notebook for Schizophrenia Detection\n",
    "\n",
    "This notebook is optimized for Google Colab and provides tools for training the SSPNet 3D CNN model for schizophrenia detection.\n",
    "\n",
    "## Features:\n",
    "- GPU configuration and memory management\n",
    "- Google Drive integration for model checkpoints\n",
    "- Progress tracking for long-running operations\n",
    "- Hyperparameter tuning interface\n",
    "- Real-time training visualization\n",
    "- Memory optimization for large 3D models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for data storage and model checkpoints\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    \n",
    "    # Check if already mounted\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "    else:\n",
    "        print(\"Google Drive already mounted\")\n",
    "    \n",
    "    # Set up project directory\n",
    "    project_path = '/content/drive/MyDrive/schizophrenia_detection'\n",
    "    os.makedirs(project_path, exist_ok=True)\n",
    "    print(f\"Project directory: {project_path}\")\n",
    "else:\n",
    "    import os\n",
    "    project_path = os.path.abspath('../')\n",
    "    print(f\"Local project directory: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages specific to Colab environment\n",
    "if IN_COLAB:\n",
    "    print(\"Installing required packages...\")\n",
    "    \n",
    "    # Core packages\n",
    "    !pip install tensorflow==2.12.0 nibabel nilearn scikit-learn matplotlib seaborn tqdm -q\n",
    "    \n",
    "    # Interactive visualization packages\n",
    "    !pip install plotly ipywidgets -q\n",
    "    \n",
    "    # Memory management and optimization packages\n",
    "    !pip install psutil -q\n",
    "    \n",
    "    # Advanced packages for hyperparameter tuning\n",
    "    !pip install optuna -q\n",
    "    \n",
    "    # Package for mixed precision training\n",
    "    !pip install tensorflow-addons -q\n",
    "    \n",
    "    print(\"Packages installed successfully!\")\n",
    "else:\n",
    "    print(\"Skipping package installation in local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GPU Configuration and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and configure memory\n",
    "if IN_COLAB:\n",
    "    import tensorflow as tf\n",
    "    from psutil import virtual_memory\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpu_available = tf.test.is_gpu_available()\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"GPU available: {gpu_available}\")\n",
    "    \n",
    "    if gpu_available:\n",
    "        gpu_name = tf.test.gpu_device_name()\n",
    "        print(f\"GPU device: {gpu_name}\")\n",
    "        \n",
    "        # Configure GPU memory growth to prevent OOM errors\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                print(\"GPU memory growth enabled\")\n",
    "                \n",
    "                # Enable mixed precision training for better performance\n",
    "                tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "                print(\"Mixed precision training enabled\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error setting GPU memory growth: {e}\")\n",
    "    \n",
    "    # Check RAM availability\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print(f\"Available RAM: {ram_gb:.2f} GB\")\n",
    "    \n",
    "    if ram_gb < 12:\n",
    "        print(\"WARNING: Low RAM detected. Consider reducing batch sizes.\")\n",
    "        # Automatically adjust batch size for low RAM\n",
    "        AUTO_BATCH_SIZE = 2\n",
    "    else:\n",
    "        AUTO_BATCH_SIZE = 4\n",
    "    \n",
    "    print(f\"Auto-configured batch size: {AUTO_BATCH_SIZE}\")\n",
    "else:\n",
    "    import tensorflow as tf\n",
    "    AUTO_BATCH_SIZE = 4\n",
    "    print(\"Skipping GPU configuration in local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities for training\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_memory_usage():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory usage: {mem_info.rss / 1e6:.2f} MB\")\n",
    "    return mem_info.rss / 1e6\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear memory by garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    if IN_COLAB:\n",
    "        tf.keras.backend.clear_session()\n",
    "    print(\"Memory cleared\")\n",
    "\n",
    "def monitor_memory(func):\n",
    "    \"\"\"Decorator to monitor memory usage of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_mem = check_memory_usage()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_mem = check_memory_usage()\n",
    "        print(f\"Memory change: {end_mem - start_mem:.2f} MB\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Progress tracking for long operations\n",
    "class TrainingProgress:\n",
    "    def __init__(self, total_epochs):\n",
    "        self.total_epochs = total_epochs\n",
    "        self.current_epoch = 0\n",
    "        self.start_time = time.time()\n",
    "        self.pbar = tqdm(total=total_epochs, desc=\"Training Progress\")\n",
    "    \n",
    "    def update(self, epoch, metrics):\n",
    "        self.current_epoch = epoch\n",
    "        elapsed = time.time() - self.start_time\n",
    "        eta = elapsed / (epoch + 1) * (self.total_epochs - epoch - 1)\n",
    "        \n",
    "        # Update progress bar with metrics\n",
    "        metric_str = \", \".join([f\"{k}: {v:.4f}\" for k, v in metrics.items()])\n",
    "        self.pbar.set_description(f\"Epoch {epoch+1}/{self.total_epochs} - {metric_str}\")\n",
    "        self.pbar.update(1)\n",
    "    \n",
    "    def close(self):\n",
    "        self.pbar.close()\n",
    "\n",
    "print(\"Memory management utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Import visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "# Configure warnings and display\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory and import project modules\n",
    "sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from config import default_config\n",
    "    from utils.file_utils import list_files, load_json, save_json\n",
    "    from utils.data_utils import normalize_data, resize_data\n",
    "    from data_processing.data_loader import create_data_generator\n",
    "    from data_processing.fmri_preprocessing import preprocess_fmri\n",
    "    from models.sspnet_3d_cnn import create_sspnet_model\n",
    "    from models.model_utils import count_parameters\n",
    "    from training.trainer import ModelTrainer\n",
    "    from training.evaluator import ModelEvaluator\n",
    "    from visualization.model_visualization import plot_training_history, ModelVisualizer\n",
    "    from visualization.result_plots import plot_confusion_matrix, plot_roc_curve\n",
    "    print(\"Project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import some project modules: {e}\")\n",
    "    print(\"Using minimal configuration for training\")\n",
    "    \n",
    "    # Create minimal configuration for testing\n",
    "    class MinimalConfig:\n",
    "        def __init__(self):\n",
    "            self.data = type('DataConfig', (), {\n",
    "                'data_root': './data',\n",
    "                'fmri_data_dir': './data/fmri',\n",
    "                'meg_data_dir': './data/meg',\n",
    "                'fmri_shape': (96, 96, 96, 4),\n",
    "                'meg_shape': (306, 100, 1000),\n",
    "                'batch_size': AUTO_BATCH_SIZE,\n",
    "                'train_ratio': 0.7,\n",
    "                'val_ratio': 0.15,\n",
    "                'test_ratio': 0.15\n",
    "            })()\n",
    "            self.model = type('ModelConfig', (), {\n",
    "                'input_shape': (96, 96, 96, 4),\n",
    "                'num_classes': 2,\n",
    "                'dropout_rate': 0.5\n",
    "            })()\n",
    "            self.training = type('TrainingConfig', (), {\n",
    "                'epochs': 50,\n",
    "                'learning_rate': 0.001,\n",
    "                'optimizer': 'adam',\n",
    "                'loss_function': 'categorical_crossentropy',\n",
    "                'early_stopping': True,\n",
    "                'patience': 15,\n",
    "                'checkpoint_dir': './checkpoints',\n",
    "                'mixed_precision': True\n",
    "            })()\n",
    "            self.evaluation = type('EvaluationConfig', (), {\n",
    "                'results_dir': './results'\n",
    "            })()\n",
    "            self.visualization = type('VisualizationConfig', (), {\n",
    "                'output_dir': './visualizations'\n",
    "            })()\n",
    "    \n",
    "    default_config = MinimalConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration cell for easy parameter adjustment\n",
    "TRAINING_CONFIG = {\n",
    "    # Model parameters\n",
    "    'input_shape': default_config.model.input_shape,\n",
    "    'num_classes': default_config.model.num_classes,\n",
    "    'dropout_rate': default_config.model.dropout_rate,\n",
    "    \n",
    "    # Training parameters\n",
    "    'epochs': default_config.training.epochs,\n",
    "    'batch_size': AUTO_BATCH_SIZE,  # Auto-configured based on available RAM\n",
    "    'learning_rate': default_config.training.learning_rate,\n",
    "    'optimizer': default_config.training.optimizer,\n",
    "    'loss_function': default_config.training.loss_function,\n",
    "    \n",
    "    # Data parameters\n",
    "    'train_ratio': default_config.data.train_ratio,\n",
    "    'val_ratio': default_config.data.val_ratio,\n",
    "    'test_ratio': default_config.data.test_ratio,\n",
    "    \n",
    "    # Training control\n",
    "    'early_stopping': default_config.training.early_stopping,\n",
    "    'patience': default_config.training.patience,\n",
    "    'save_best_only': True,\n",
    "    \n",
    "    # Memory management\n",
    "    'use_mixed_precision': default_config.training.mixed_precision,\n",
    "    'clear_memory_between_epochs': True,\n",
    "    'use_data_augmentation': True,\n",
    "    \n",
    "    # Checkpointing\n",
    "    'checkpoint_dir': default_config.training.checkpoint_dir,\n",
    "    'save_to_drive': IN_COLAB,\n",
    "    \n",
    "    # Hyperparameter tuning\n",
    "    'enable_hyperparameter_tuning': False,\n",
    "    'tuning_trials': 20,\n",
    "    \n",
    "    # Visualization\n",
    "    'real_time_plotting': True,\n",
    "    'plot_frequency': 5  # Plot every N epochs\n",
    "}\n",
    "\n",
    "# Update configuration based on available resources\n",
    "if IN_COLAB:\n",
    "    # Adjust for Colab environment\n",
    "    if TRAINING_CONFIG['batch_size'] > 2:\n",
    "        TRAINING_CONFIG['batch_size'] = 2\n",
    "        print(\"Reduced batch size for Colab environment\")\n",
    "    \n",
    "    # Reduce epochs for faster demonstration\n",
    "    if TRAINING_CONFIG['epochs'] > 30:\n",
    "        TRAINING_CONFIG['epochs'] = 30\n",
    "        print(\"Reduced epochs for faster demonstration\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(TRAINING_CONFIG['checkpoint_dir'], exist_ok=True)\n",
    "os.makedirs(default_config.evaluation.results_dir, exist_ok=True)\n",
    "os.makedirs(default_config.visualization.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Training configuration set:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata_path = os.path.join(default_config.data.data_root, 'metadata.csv')\n",
    "\n",
    "if os.path.exists(metadata_path):\n",
    "    print(f\"Loading metadata from: {metadata_path}\")\n",
    "    metadata = pd.read_csv(metadata_path)\n",
    "    print(f\"Metadata shape: {metadata.shape}\")\n",
    "    display(metadata.head())\n",
    "    \n",
    "    # Check class distribution\n",
    "    if 'diagnosis' in metadata.columns:\n",
    "        print(\"\\nDiagnosis distribution:\")\n",
    "        diagnosis_counts = metadata['diagnosis'].value_counts()\n",
    "        print(diagnosis_counts)\n",
    "        \n",
    "        # Plot class distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        diagnosis_counts.plot(kind='bar', color=['skyblue', 'lightcoral'])\n",
    "        plt.title('Class Distribution')\n",
    "        plt.xlabel('Diagnosis')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No metadata file found. Creating sample metadata for demonstration.\")\n",
    "    \n",
    "    # Create sample metadata for demonstration\n",
    "    from utils.file_utils import list_files\n",
    "    fmri_files = list_files(default_config.data.fmri_data_dir, '.nii.gz')\n",
    "    \n",
    "    if not fmri_files:\n",
    "        print(\"No fMRI files found. Creating sample data for demonstration.\")\n",
    "        os.makedirs(default_config.data.fmri_data_dir, exist_ok=True)\n",
    "        \n",
    "        # Create sample fMRI files\n",
    "        import nibabel as nibabel\n",
    "        for i in range(20):  # Create 20 sample subjects\n",
    "            sample_shape = default_config.data.fmri_shape\n",
    "            sample_data = np.random.randn(*sample_shape)\n",
    "            sample_img = nibabel.Nifti1Image(sample_data, affine=np.eye(4))\n",
    "            \n",
    "            sample_path = os.path.join(default_config.data.fmri_data_dir, f'sub-{i:03d}_fmri.nii.gz')\n",
    "            nibabel.save(sample_img, sample_path)\n",
    "        \n",
    "        fmri_files = list_files(default_config.data.fmri_data_dir, '.nii.gz')\n",
    "    \n",
    "    # Create sample labels (0 for control, 1 for schizophrenia)\n",
    "    labels = np.random.randint(0, 2, size=len(fmri_files))\n",
    "    \n",
    "    # Create metadata DataFrame\n",
    "    metadata = pd.DataFrame({\n",
    "        'subject_id': [f\"sub-{i:03d}\" for i in range(len(fmri_files))],\n",
    "        'file_path': fmri_files,\n",
    "        'diagnosis': ['control' if label == 0 else 'schizophrenia' for label in labels],\n",
    "        'label': labels,\n",
    "        'age': np.random.randint(18, 65, len(fmri_files)),\n",
    "        'gender': np.random.choice(['M', 'F'], len(fmri_files))\n",
    "    })\n",
    "    \n",
    "    # Save sample metadata\n",
    "    os.makedirs(default_config.data.data_root, exist_ok=True)\n",
    "    metadata.to_csv(metadata_path, index=False)\n",
    "    print(f\"Created sample metadata with {len(metadata)} subjects: {metadata_path}\")\n",
    "    \n",
    "    display(metadata.head())\n",
    "    print(f\"\\nDiagnosis distribution:\")\n",
    "    print(metadata['diagnosis'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert diagnosis to numeric labels if needed\n",
    "if 'label' not in metadata.columns:\n",
    "    label_map = {'control': 0, 'schizophrenia': 1}\n",
    "    metadata['label'] = metadata['diagnosis'].map(label_map)\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "print(\"Splitting data into train, validation, and test sets...\")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    metadata, \n",
    "    test_size=(1 - TRAINING_CONFIG['train_ratio']),\n",
    "    random_state=42,\n",
    "    stratify=metadata['label']\n",
    ")\n",
    "\n",
    "val_ratio = TRAINING_CONFIG['val_ratio'] / (1 - TRAINING_CONFIG['train_ratio'])\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=(1 - val_ratio),\n",
    "    random_state=42,\n",
    "    stratify=temp_df['label']\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} subjects\")\n",
    "print(f\"Validation set: {len(val_df)} subjects\")\n",
    "print(f\"Test set: {len(test_df)} subjects\")\n",
    "\n",
    "print(\"\\nTraining set distribution:\")\n",
    "print(train_df['diagnosis'].value_counts())\n",
    "print(\"\\nValidation set distribution:\")\n",
    "print(val_df['diagnosis'].value_counts())\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(test_df['diagnosis'].value_counts())\n",
    "\n",
    "# Calculate class weights for imbalanced datasets\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(train_df['label']),\n",
    "    y=train_df['label']\n",
    ")\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nibabel for fMRI data loading\n",
    "try:\n",
    "    import nibabel as nib\n",
    "except ImportError:\n",
    "    print(\"Installing nibabel...\")\n",
    "    !pip install nibabel -q\n",
    "    import nibabel as nib\n",
    "\n",
    "# Memory-efficient data loading function\n",
    "@monitor_memory\n",
    "def load_and_preprocess_fmri(file_path, target_shape=None, normalize=True):\n",
    "    \"\"\"Load and preprocess fMRI data with memory management\"\"\"\n",
    "    try:\n",
    "        # Load fMRI data with memory mapping\n",
    "        img = nib.load(file_path, mmap='r+')\n",
    "        data = img.get_fdata()\n",
    "        \n",
    "        # Handle 4D data (take first few time points if needed)\n",
    "        if len(data.shape) == 4:\n",
    "            if data.shape[3] > target_shape[3] if target_shape else data.shape[3] > 4:\n",
    "                # Take first few time points\n",
    "                n_timepoints = target_shape[3] if target_shape else 4\n",
    "                data = data[..., :n_timepoints]\n",
    "        elif len(data.shape) == 3:\n",
    "            # Add time dimension if 3D\n",
    "            n_timepoints = target_shape[3] if target_shape else 4\n",
    "            data = np.expand_dims(data, axis=-1)\n",
    "            # Repeat to match required time points\n",
    "            data = np.repeat(data, n_timepoints, axis=-1)\n",
    "        \n",
    "        # Resize if needed\n",
    "        if target_shape and data.shape != target_shape:\n",
    "            try:\n",
    "                from utils.data_utils import resize_data\n",
    "                data = resize_data(data, target_shape)\n",
    "            except:\n",
    "                print(f\"Warning: Could not resize data from {data.shape} to {target_shape}\")\n",
    "        \n",
    "        # Normalize data\n",
    "        if normalize:\n",
    "            try:\n",
    "                from utils.data_utils import normalize_data\n",
    "                data = normalize_data(data, method='standard')\n",
    "            except:\n",
    "                # Simple z-score normalization\n",
    "                data = (data - np.mean(data)) / (np.std(data) + 1e-8)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data generators with memory management\n",
    "def create_memory_efficient_generator(file_paths, labels, batch_size, training=True, shuffle=True):\n",
    "    \"\"\"Create memory-efficient data generator for 3D neuroimaging data\"\"\"\n",
    "    num_samples = len(file_paths)\n",
    "    \n",
    "    while True:\n",
    "        # Create indices for batching\n",
    "        indices = np.random.permutation(num_samples) if shuffle and training else np.arange(num_samples)\n",
    "        \n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_size_actual = len(batch_indices)\n",
    "            \n",
    "            # Initialize batch arrays\n",
    "            batch_x = np.zeros((batch_size_actual,) + TRAINING_CONFIG['input_shape'], dtype=np.float32)\n",
    "            batch_y = np.zeros((batch_size_actual, TRAINING_CONFIG['num_classes']), dtype=np.float32)\n",
    "            \n",
    "            valid_samples = 0\n",
    "            for j, idx in enumerate(batch_indices):\n",
    "                try:\n",
    "                    # Load and preprocess data\n",
    "                    data = load_and_preprocess_fmri(\n",
    "                        file_paths[idx], \n",
    "                        target_shape=TRAINING_CONFIG['input_shape'],\n",
    "                        normalize=True\n",
    "                    )\n",
    "                    \n",
    "                    if data is not None:\n",
    "                        batch_x[valid_samples] = data\n",
    "                        \n",
    "                        # One-hot encode label\n",
    "                        batch_y[valid_samples, labels[idx]] = 1\n",
    "                        valid_samples += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_paths[idx]}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Only yield if we have valid samples\n",
    "            if valid_samples > 0:\n",
    "                yield batch_x[:valid_samples], batch_y[:valid_samples]\n",
    "            \n",
    "            # Clear memory between batches if enabled\n",
    "            if TRAINING_CONFIG['clear_memory_between_epochs']:\n",
    "                if i % (batch_size * 5) == 0:  # Clear every 5 batches\n",
    "                    gc.collect()\n",
    "\n",
    "# Create data generators\n",
    "print(\"Creating data generators...\")\n",
    "\n",
    "train_generator = create_memory_efficient_generator(\n",
    "    train_df['file_path'].tolist(),\n",
    "    train_df['label'].tolist(),\n",
    "    TRAINING_CONFIG['batch_size'],\n",
    "    training=True,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = create_memory_efficient_generator(\n",
    "    val_df['file_path'].tolist(),\n",
    "    val_df['label'].tolist(),\n",
    "    TRAINING_CONFIG['batch_size'],\n",
    "    training=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = create_memory_efficient_generator(\n",
    "    test_df['file_path'].tolist(),\n",
    "    test_df['label'].tolist(),\n",
    "    TRAINING_CONFIG['batch_size'],\n",
    "    training=False,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Data generators created successfully\")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(train_df) // TRAINING_CONFIG['batch_size']\n",
    "validation_steps = len(val_df) // TRAINING_CONFIG['batch_size']\n",
    "test_steps = len(test_df) // TRAINING_CONFIG['batch_size']\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "print(f\"Test steps: {test_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the SSPNet 3D CNN model\n",
    "print(\"Building SSPNet 3D CNN model...\")\n",
    "\n",
    "try:\n",
    "    # Try to use the project's model creation function\n",
    "    model = create_sspnet_model(\n",
    "        input_shape=TRAINING_CONFIG['input_shape'],\n",
    "        num_classes=TRAINING_CONFIG['num_classes'],\n",
    "        dropout_rate=TRAINING_CONFIG['dropout_rate']\n",
    "    )\n",
    "    print(\"Model created using project's SSPNet architecture\")\n",
    "except:\n",
    "    print(\"Could not load project model. Creating a simplified 3D CNN model...\")\n",
    "    \n",
    "    # Create a simplified 3D CNN model for demonstration\n",
    "    from tensorflow.keras import layers, models, regularizers\n",
    "    \n",
    "    def create_simple_3d_cnn(input_shape, num_classes, dropout_rate=0.5):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        \n",
    "        # First 3D convolution block\n",
    "        x = layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "        x = layers.Dropout(dropout_rate * 0.5)(x)\n",
    "        \n",
    "        # Second 3D convolution block\n",
    "        x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "        x = layers.Dropout(dropout_rate * 0.5)(x)\n",
    "        \n",
    "        # Third 3D convolution block\n",
    "        x = layers.Conv3D(128, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        # Create model\n",
    "        model = models.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    model = create_simple_3d_cnn(\n",
    "        input_shape=TRAINING_CONFIG['input_shape'],\n",
    "        num_classes=TRAINING_CONFIG['num_classes'],\n",
    "        dropout_rate=TRAINING_CONFIG['dropout_rate']\n",
    "    )\n",
    "\n",
    "# Print model summary\n",
    "print(\"Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "try:\n",
    "    total_params = count_parameters(model)\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "except:\n",
    "    total_params = model.count_params()\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "\n",
    "# Calculate model memory usage\n",
    "param_memory = total_params * 4  # 4 bytes per float32\n",
    "print(f\"Estimated model memory: {param_memory / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with appropriate optimizer and loss\n",
    "print(\"Compiling model...\")\n",
    "\n",
    "# Configure optimizer based on mixed precision setting\n",
    "if TRAINING_CONFIG['use_mixed_precision']:\n",
    "    optimizer = tf.keras.mixed_precision.LossScaleOptimizer(\n",
    "        tf.keras.optimizers.Adam(learning_rate=TRAINING_CONFIG['learning_rate'])\n",
    "    )\n",
    "    print(\"Using mixed precision optimizer\")\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=TRAINING_CONFIG['learning_rate'])\n",
    "    print(\"Using standard precision optimizer\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=TRAINING_CONFIG['loss_function'],\n",
    "    metrics=['accuracy', 'AUC']\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"Loss function: {TRAINING_CONFIG['loss_function']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Callbacks Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, \n",
    "    TensorBoard, CSVLogger, Callback\n",
    ")\n",
    "\n",
    "# Custom callback for real-time plotting\n",
    "class RealTimePlotCallback(Callback):\n",
    "    def __init__(self, plot_frequency=5):\n",
    "        super().__init__()\n",
    "        self.plot_frequency = plot_frequency\n",
    "        self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Update history\n",
    "        for key in self.history.keys():\n",
    "            if key in logs:\n",
    "                self.history[key].append(logs[key])\n",
    "        \n",
    "        # Plot every N epochs\n",
    "        if (epoch + 1) % self.plot_frequency == 0:\n",
    "            self.plot_training_progress()\n",
    "    \n",
    "    def plot_training_progress(self):\n",
    "        if len(self.history['loss']) < 2:\n",
    "            return\n",
    "            \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot loss\n",
    "        axes[0].plot(self.history['loss'], label='Training Loss', color='blue')\n",
    "        if 'val_loss' in self.history and self.history['val_loss']:\n",
    "            axes[0].plot(self.history['val_loss'], label='Validation Loss', color='red')\n",
    "        axes[0].set_title('Model Loss')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Loss')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        axes[1].plot(self.history['accuracy'], label='Training Accuracy', color='blue')\n",
    "        if 'val_accuracy' in self.history and self.history['val_accuracy']:\n",
    "            axes[1].plot(self.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
    "        axes[1].set_title('Model Accuracy')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create callbacks list\n",
    "callbacks = []\n",
    "\n",
    "# Model checkpoint callback\n",
    "checkpoint_path = os.path.join(\n",
    "    TRAINING_CONFIG['checkpoint_dir'], \n",
    "    'sspnet_best_model.h5'\n",
    ")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    checkpoint_path,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=TRAINING_CONFIG['save_best_only'],\n",
    "    save_weights_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "callbacks.append(checkpoint_callback)\n",
    "\n",
    "# Early stopping callback\n",
    "if TRAINING_CONFIG['early_stopping']:\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=TRAINING_CONFIG['patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping_callback)\n",
    "\n",
    "# Learning rate reduction callback\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "callbacks.append(reduce_lr_callback)\n",
    "\n",
    "# CSV logger callback\n",
    "csv_logger_path = os.path.join(\n",
    "    default_config.evaluation.results_dir, \n",
    "    'training_log.csv'\n",
    ")\n",
    "csv_logger_callback = CSVLogger(csv_logger_path)\n",
    "callbacks.append(csv_logger_callback)\n",
    "\n",
    "# Real-time plotting callback\n",
    "if TRAINING_CONFIG['real_time_plotting']:\n",
    "    plot_callback = RealTimePlotCallback(plot_frequency=TRAINING_CONFIG['plot_frequency'])\n",
    "    callbacks.append(plot_callback)\n",
    "\n",
    "# TensorBoard callback (for Colab)\n",
    "if IN_COLAB:\n",
    "    tensorboard_callback = TensorBoard(\n",
    "        log_dir=os.path.join(default_config.training.checkpoint_dir, 'tensorboard_logs'),\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        update_freq='epoch'\n",
    "    )\n",
    "    callbacks.append(tensorboard_callback)\n",
    "    \n",
    "    # Load TensorBoard extension\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir {os.path.join(default_config.training.checkpoint_dir, 'tensorboard_logs')}\n",
    "\n",
    "print(f\"Callbacks configured: {len(callbacks)} callbacks\")\n",
    "for callback in callbacks:\n",
    "    print(f\"  - {callback.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Progress Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize progress tracking\n",
    "progress_tracker = TrainingProgress(TRAINING_CONFIG['epochs'])\n",
    "\n",
    "# Custom training loop with memory management\n",
    "class MemoryEfficientTrainer:\n",
    "    def __init__(self, model, train_gen, val_gen, steps_per_epoch, val_steps, config):\n",
    "        self.model = model\n",
    "        self.train_gen = train_gen\n",
    "        self.val_gen = val_gen\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.val_steps = val_steps\n",
    "        self.config = config\n",
    "        self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "        \n",
    "    def train_epoch(self, epoch):\n",
    "        \"\"\"Train for one epoch with memory management\"\"\"\n",
    "        # Initialize metrics\n",
    "        epoch_loss = 0\n",
    "        epoch_accuracy = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for step in range(self.steps_per_epoch):\n",
    "            # Get batch data\n",
    "            batch_x, batch_y = next(self.train_gen)\n",
    "            \n",
    "            # Train on batch\n",
    "            metrics = self.model.train_on_batch(batch_x, batch_y)\n",
    "            \n",
    "            # Update metrics\n",
    "            if isinstance(metrics, list):\n",
    "                epoch_loss += metrics[0]\n",
    "                epoch_accuracy += metrics[1]\n",
    "            else:\n",
    "                epoch_loss += metrics\n",
    "                epoch_accuracy += 0  # No accuracy metric\n",
    "            \n",
    "            num_batches += 1\n",
    "            \n",
    "            # Clear memory periodically\n",
    "            if step % 10 == 0 and self.config['clear_memory_between_epochs']:\n",
    "                gc.collect()\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        avg_accuracy = epoch_accuracy / num_batches\n",
    "        \n",
    "        return avg_loss, avg_accuracy\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        # Initialize metrics\n",
    "        val_loss = 0\n",
    "        val_accuracy = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Validation loop\n",
    "        for step in range(self.val_steps):\n",
    "            # Get batch data\n",
    "            batch_x, batch_y = next(self.val_gen)\n",
    "            \n",
    "            # Validate on batch\n",
    "            metrics = self.model.test_on_batch(batch_x, batch_y)\n",
    "            \n",
    "            # Update metrics\n",
    "            if isinstance(metrics, list):\n",
    "                val_loss += metrics[0]\n",
    "                val_accuracy += metrics[1]\n",
    "            else:\n",
    "                val_loss += metrics\n",
    "                val_accuracy += 0  # No accuracy metric\n",
    "            \n",
    "            num_batches += 1\n",
    "        \n",
    "        # Calculate average metrics\n",
    "        avg_val_loss = val_loss / num_batches\n",
    "        avg_val_accuracy = val_accuracy / num_batches\n",
    "        \n",
    "        return avg_val_loss, avg_val_accuracy\n",
    "    \n",
    "    def train(self, epochs, callbacks=None):\n",
    "        \"\"\"Train the model with custom loop\"\"\"\n",
    "        print(f\"Starting training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "            \n",
    "            # Train epoch\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            \n",
    "            # Validate epoch\n",
    "            val_loss, val_acc = self.validate_epoch()\n",
    "            \n",
    "            # Update history\n",
    "            self.history['loss'].append(train_loss)\n",
    "            self.history['accuracy'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_accuracy'].append(val_acc)\n",
    "            \n",
    "            # Update progress tracker\n",
    "            metrics = {\n",
    "                'loss': train_loss,\n",
    "                'accuracy': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': val_acc\n",
    "            }\n",
    "            progress_tracker.update(epoch, metrics)\n",
    "            \n",
    "            # Print epoch summary\n",
    "            print(f\"loss: {train_loss:.4f} - accuracy: {train_acc:.4f} - \")\n",
    "            print(f\"val_loss: {val_loss:.4f} - val_accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            # Check early stopping\n",
    "            if len(self.history['val_accuracy']) > self.config['patience']:\n",
    "                recent_val_acc = self.history['val_accuracy'][-self.config['patience']:]\n",
    "                if max(recent_val_acc) <= self.history['val_accuracy'][-self.config['patience']-1]:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "        \n",
    "        progress_tracker.close()\n",
    "        print(\"\\nTraining completed!\")\n",
    "        return self.history\n",
    "\n",
    "print(\"Custom training utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose training method based on configuration\n",
    "use_custom_training = False  # Set to True for custom training loop\n",
    "\n",
    "if use_custom_training:\n",
    "    print(\"Using custom training loop with memory management...\")\n",
    "    \n",
    "    # Create custom trainer\n",
    "    trainer = MemoryEfficientTrainer(\n",
    "        model=model,\n",
    "        train_gen=train_generator,\n",
    "        val_gen=val_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        val_steps=validation_steps,\n",
    "        config=TRAINING_CONFIG\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    history = trainer.train(\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"Using standard Keras fit method...\")\n",
    "    \n",
    "    # Train model using standard fit method\n",
    "    print(\"\\nStarting model training...\")\n",
    "    print(f\"Training for {TRAINING_CONFIG['epochs']} epochs\")\n",
    "    print(f\"Batch size: {TRAINING_CONFIG['batch_size']}\")\n",
    "    print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "    \n",
    "    # Check memory before training\n",
    "    print(\"\\nMemory usage before training:\")\n",
    "    check_memory_usage()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weight_dict,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTraining completed!\")\n",
    "    \n",
    "    # Check memory after training\n",
    "    print(\"\\nMemory usage after training:\")\n",
    "    check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if use_custom_training:\n",
    "    # Use custom training history\n",
    "    training_history = history\n",
    "else:\n",
    "    # Use Keras training history\n",
    "    training_history = history.history\n",
    "\n",
    "# Create comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot training loss\n",
    "axes[0, 0].plot(training_history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "if 'val_loss' in training_history:\n",
    "    axes[0, 0].plot(training_history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot training accuracy\n",
    "axes[0, 1].plot(training_history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
    "if 'val_accuracy' in training_history:\n",
    "    axes[0, 1].plot(training_history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
    "axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning curves (loss comparison)\n",
    "if 'val_loss' in training_history:\n",
    "    axes[1, 0].plot(training_history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "    axes[1, 0].plot(training_history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[1, 0].set_title('Learning Curves', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight best epoch\n",
    "    best_val_loss_epoch = np.argmin(training_history['val_loss'])\n",
    "    best_val_loss = training_history['val_loss'][best_val_loss_epoch]\n",
    "    axes[1, 0].scatter(best_val_loss_epoch, best_val_loss, color='green', s=100, zorder=5)\n",
    "    axes[1, 0].annotate(f'Best: Epoch {best_val_loss_epoch+1}', \n",
    "                        xy=(best_val_loss_epoch, best_val_loss),\n",
    "                        xytext=(best_val_loss_epoch+1, best_val_loss*1.1),\n",
    "                        arrowprops=dict(arrowstyle='->', color='green'))\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'No validation data available', \n",
    "                   ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "    axes[1, 0].set_title('Learning Curves')\n",
    "\n",
    "# Plot training progress summary\n",
    "if len(training_history['loss']) > 0:\n",
    "    final_train_loss = training_history['loss'][-1]\n",
    "    final_train_acc = training_history['accuracy'][-1]\n",
    "    \n",
    "    summary_text = f\"Training Summary:\\n\"\n",
    "    summary_text += f\"Final Loss: {final_train_loss:.4f}\\n\"\n",
    "    summary_text += f\"Final Accuracy: {final_train_acc:.4f}\\n\"\n",
    "    \n",
    "    if 'val_loss' in training_history:\n",
    "        final_val_loss = training_history['val_loss'][-1]\n",
    "        final_val_acc = training_history['val_accuracy'][-1]\n",
    "        summary_text += f\"Final Val Loss: {final_val_loss:.4f}\\n\"\n",
    "        summary_text += f\"Final Val Accuracy: {final_val_acc:.4f}\\n\"\n",
    "        \n",
    "        # Calculate overfitting metric\n",
    "        overfitting = final_train_acc - final_val_acc\n",
    "        summary_text += f\"Overfitting: {overfitting:.4f}\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, summary_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=12, verticalalignment='center',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[1, 1].set_title('Training Summary', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save training plots\n",
    "training_plot_path = os.path.join(default_config.visualization.output_dir, 'training_history.png')\n",
    "plt.savefig(training_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Training plots saved to {training_plot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "\n",
    "# Load best model\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading best model from {checkpoint_path}\")\n",
    "    model.load_weights(checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found. Using current model weights.\")\n",
    "\n",
    "# Collect predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "for i in tqdm(range(test_steps), desc=\"Evaluating\"):\n",
    "    batch_x, batch_y = next(test_generator)\n",
    "    batch_pred = model.predict(batch_x, verbose=0)\n",
    "    \n",
    "    y_true.extend(np.argmax(batch_y, axis=1))\n",
    "    y_pred.extend(np.argmax(batch_pred, axis=1))\n",
    "    y_pred_proba.extend(batch_pred[:, 1])  # Probability of positive class\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "print(f\"\\nEvaluated {len(y_true)} samples\")\n",
    "\n",
    "# Calculate metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "auc = roc_auc_score(y_true, y_pred_proba)\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "target_names = ['Control', 'Schizophrenia']\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()\n",
    "\n",
    "# Save confusion matrix\n",
    "cm_path = os.path.join(default_config.visualization.output_dir, 'confusion_matrix.png')\n",
    "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Confusion matrix saved to {cm_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Save ROC curve\n",
    "roc_path = os.path.join(default_config.visualization.output_dir, 'roc_curve.png')\n",
    "plt.savefig(roc_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"ROC curve saved to {roc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "final_model_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'sspnet_final_model.h5')\n",
    "model.save(final_model_path)\n",
    "print(f\"Final model saved to {final_model_path}\")\n",
    "\n",
    "# Save model architecture as JSON\n",
    "model_json_path = os.path.join(TRAINING_CONFIG['checkpoint_dir'], 'sspnet_model_architecture.json')\n",
    "with open(model_json_path, 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "print(f\"Model architecture saved to {model_json_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = os.path.join(default_config.evaluation.results_dir, 'training_history.json')\n",
    "with open(history_path, 'w') as f:\n",
    "    json.dump(training_history, f, indent=2)\n",
    "print(f\"Training history saved to {history_path}\")\n",
    "\n",
    "# Save test results\n",
    "test_results = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'precision': float(precision),\n",
    "    'recall': float(recall),\n",
    "    'f1_score': float(f1),\n",
    "    'auc': float(auc),\n",
    "    'confusion_matrix': cm.tolist(),\n",
    "    'classification_report': classification_report(y_true, y_pred, target_names=target_names, output_dict=True)\n",
    "}\n",
    "\n",
    "test_results_path = os.path.join(default_config.evaluation.results_dir, 'test_results.json')\n",
    "with open(test_results_path, 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "print(f\"Test results saved to {test_results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to Google Drive\n",
    "if IN_COLAB and TRAINING_CONFIG['save_to_drive']:\n",
    "    print(\"\\nExporting results to Google Drive...\")\n",
    "    \n",
    "    import shutil\n",
    "    \n",
    "    # Create timestamped directory in Google Drive\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_export_dir = os.path.join(project_path, f'training_results_{timestamp}')\n",
    "    os.makedirs(drive_export_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy model files\n",
    "    shutil.copy2(final_model_path, drive_export_dir)\n",
    "    shutil.copy2(model_json_path, drive_export_dir)\n",
    "    \n",
    "    # Copy results files\n",
    "    shutil.copy2(history_path, drive_export_dir)\n",
    "    shutil.copy2(test_results_path, drive_export_dir)\n",
    "    shutil.copy2(csv_logger_path, drive_export_dir)\n",
    "    \n",
    "    # Copy visualization files\n",
    "    viz_export_dir = os.path.join(drive_export_dir, 'visualizations')\n",
    "    os.makedirs(viz_export_dir, exist_ok=True)\n",
    "    \n",
    "    for viz_file in [training_plot_path, cm_path, roc_path]:\n",
    "        if os.path.exists(viz_file):\n",
    "            shutil.copy2(viz_file, viz_export_dir)\n",
    "    \n",
    "    print(f\"Results exported to Google Drive: {drive_export_dir}\")\n",
    "    \n",
    "    # Create summary file\n",
    "    summary = {\n",
    "        'training_completed': datetime.now().isoformat(),\n",
    "        'model_architecture': 'SSPNet 3D CNN',\n",
    "        'training_config': TRAINING_CONFIG,\n",
    "        'final_metrics': test_results,\n",
    "        'files_exported': {\n",
    "            'model': 'sspnet_final_model.h5',\n",
    "            'architecture': 'sspnet_model_architecture.json',\n",
    "            'training_history': 'training_history.json',\n",
    "            'test_results': 'test_results.json',\n",
    "            'training_log': 'training_log.csv'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(drive_export_dir, 'training_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Training summary saved to {summary_path}\")\n",
    "else:\n",
    "    print(\"\\nResults saved locally. Google Drive export skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter tuning with Optuna\n",
    "if TRAINING_CONFIG['enable_hyperparameter_tuning']:\n",
    "    print(\"\\nStarting hyperparameter tuning...\")\n",
    "    \n",
    "    try:\n",
    "        import optuna\n",
    "    except ImportError:\n",
    "        print(\"Installing Optuna for hyperparameter tuning...\")\n",
    "        !pip install optuna -q\n",
    "        import optuna\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Objective function for hyperparameter optimization\"\"\"\n",
    "        # Define hyperparameters to tune\n",
    "        hp = {\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "            'dropout_rate': trial.suggest_float('dropout_rate', 0.2, 0.7),\n",
    "            'batch_size': trial.suggest_categorical('batch_size', [2, 4]),\n",
    "            'epochs': trial.suggest_int('epochs', 10, 30)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nTrial {trial.number}: {hp}\")\n",
    "        \n",
    "        # Create model with hyperparameters\n",
    "        try:\n",
    "            tuned_model = create_sspnet_model(\n",
    "                input_shape=TRAINING_CONFIG['input_shape'],\n",
    "                num_classes=TRAINING_CONFIG['num_classes'],\n",
    "                dropout_rate=hp['dropout_rate']\n",
    "            )\n",
    "        except:\n",
    "            tuned_model = create_simple_3d_cnn(\n",
    "                input_shape=TRAINING_CONFIG['input_shape'],\n",
    "                num_classes=TRAINING_CONFIG['num_classes'],\n",
    "                dropout_rate=hp['dropout_rate']\n",
    "            )\n",
    "        \n",
    "        # Compile model\n",
    "        tuned_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=hp['learning_rate']),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Create data generators with new batch size\n",
    "        tuned_train_gen = create_memory_efficient_generator(\n",
    "            train_df['file_path'].tolist(),\n",
    "            train_df['label'].tolist(),\n",
    "            hp['batch_size'],\n",
    "            training=True,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        tuned_val_gen = create_memory_efficient_generator(\n",
    "            val_df['file_path'].tolist(),\n",
    "            val_df['label'].tolist(),\n",
    "            hp['batch_size'],\n",
    "            training=False,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Calculate steps\n",
    "        tuned_steps = len(train_df) // hp['batch_size']\n",
    "        tuned_val_steps = len(val_df) // hp['batch_size']\n",
    "        \n",
    "        # Train model\n",
    "        tuned_history = tuned_model.fit(\n",
    "            tuned_train_gen,\n",
    "            steps_per_epoch=tuned_steps,\n",
    "            epochs=hp['epochs'],\n",
    "            validation_data=tuned_val_gen,\n",
    "            validation_steps=tuned_val_steps,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Get best validation accuracy\n",
    "        best_val_acc = max(tuned_history.history['val_accuracy'])\n",
    "        \n",
    "        print(f\"Trial {trial.number} - Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        clear_memory()\n",
    "        \n",
    "        return best_val_acc\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    \n",
    "    # Optimize\n",
    "    study.optimize(objective, n_trials=TRAINING_CONFIG['tuning_trials'])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== HYPERPARAMETER TUNING RESULTS ===\")\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best value: {study.best_value:.4f}\")\n",
    "    print(\"Best parameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save tuning results\n",
    "    tuning_results = {\n",
    "        'best_trial': study.best_trial.number,\n",
    "        'best_value': study.best_value,\n",
    "        'best_params': study.best_params,\n",
    "        'all_trials': [\n",
    "            {\n",
    "                'number': trial.number,\n",
    "                'value': trial.value,\n",
    "                'params': trial.params\n",
    "            } for trial in study.trials\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    tuning_results_path = os.path.join(default_config.evaluation.results_dir, 'hyperparameter_tuning_results.json')\n",
    "    with open(tuning_results_path, 'w') as f:\n",
    "        json.dump(tuning_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nTuning results saved to {tuning_results_path}\")\n",
    "    \n",
    "    # Plot optimization history\n",
    "    try:\n",
    "        optuna.visualization.plot_optimization_history(study).show()\n",
    "        optuna.visualization.plot_param_importances(study).show()\n",
    "    except:\n",
    "        print(\"Could not display Optuna visualizations\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nHyperparameter tuning disabled. Set 'enable_hyperparameter_tuning': True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCHIZOPHRENIA DETECTION MODEL TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n🔧 Training Configuration:\")\n",
    "print(f\"  - Model Architecture: SSPNet 3D CNN\")\n",
    "print(f\"  - Input Shape: {TRAINING_CONFIG['input_shape']}\")\n",
    "print(f\"  - Batch Size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"  - Epochs Trained: {len(training_history['loss'])}\")\n",
    "print(f\"  - Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Optimizer: {TRAINING_CONFIG['optimizer']}\")\n",
    "print(f\"  - Mixed Precision: {TRAINING_CONFIG['use_mixed_precision']}\")\n",
    "\n",
    "print(f\"\\n📊 Training Results:\")\n",
    "print(f\"  - Final Training Loss: {training_history['loss'][-1]:.4f}\")\n",
    "print(f\"  - Final Training Accuracy: {training_history['accuracy'][-1]:.4f}\")\n",
    "if 'val_loss' in training_history:\n",
    "    print(f\"  - Final Validation Loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"  - Final Validation Accuracy: {training_history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n🧪 Test Set Performance:\")\n",
    "print(f\"  - Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  - Precision: {precision:.4f}\")\n",
    "print(f\"  - Recall: {recall:.4f}\")\n",
    "print(f\"  - F1 Score: {f1:.4f}\")\n",
    "print(f\"  - AUC: {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n💾 Generated Files:\")\n",
    "print(f\"  - Model: {final_model_path}\")\n",
    "print(f\"  - Architecture: {model_json_path}\")\n",
    "print(f\"  - Training History: {history_path}\")\n",
    "print(f\"  - Test Results: {test_results_path}\")\n",
    "print(f\"  - Visualizations: {default_config.visualization.output_dir}\")\n",
    "\n",
    "if IN_COLAB and TRAINING_CONFIG['save_to_drive']:\n",
    "    print(f\"  - Google Drive Export: {drive_export_dir}\")\n",
    "\n",
    "print(f\"\\n📈 Model Performance Assessment:\")\n",
    "if accuracy > 0.8:\n",
    "    print(\"  ✅ Excellent performance (>80% accuracy)\")\n",
    "elif accuracy > 0.7:\n",
    "    print(\"  ✅ Good performance (>70% accuracy)\")\n",
    "elif accuracy > 0.6:\n",
    "    print(\"  ⚠️ Moderate performance (>60% accuracy)\")\n",
    "else:\n",
    "    print(\"  ❌ Poor performance (<60% accuracy)\")\n",
    "\n",
    "# Check for overfitting\n",
    "if 'val_accuracy' in training_history:\n",
    "    overfitting = training_history['accuracy'][-1] - training_history['val_accuracy'][-1]\n",
    "    if overfitting > 0.1:\n",
    "        print(f\"  ⚠️ Potential overfitting detected (gap: {overfitting:.3f})\")\n",
    "    else:\n",
    "        print(f\"  ✅ Good generalization (gap: {overfitting:.3f})\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps:\")\n",
    "print(f\"  1. Run the results_analysis.ipynb notebook for detailed evaluation\")\n",
    "print(f\"  2. Use the trained model for inference on new data\")\n",
    "print(f\"  3. Consider hyperparameter tuning for better performance\")\n",
    "print(f\"  4. Implement cross-validation for more robust evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory before ending\n",
    "print(\"\\n🧹 Cleaning up memory...\")\n",
    "clear_memory()\n",
    "check_memory_usage()\n",
    "\n",
    "print(\"\\n✅ Model training notebook completed successfully!\")\n",
    "print(\"\\nModel is ready for use in schizophrenia detection tasks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}