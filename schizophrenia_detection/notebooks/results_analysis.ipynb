{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analysis Notebook for Schizophrenia Detection\n",
    "\n",
    "This notebook is optimized for Google Colab and provides tools for analyzing and visualizing the results of the schizophrenia detection model.\n",
    "\n",
    "## Features:\n",
    "- Comprehensive model evaluation\n",
    "- Interactive visualization dashboards\n",
    "- Brain visualization (Grad-CAM, saliency maps)\n",
    "- Comparative analysis with baseline models\n",
    "- Publication-ready figure generation\n",
    "- Google Drive integration for result storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Google Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for data storage and model checkpoints\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    \n",
    "    # Check if already mounted\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"Mounting Google Drive...\")\n",
    "        drive.mount('/content/drive')\n",
    "    else:\n",
    "        print(\"Google Drive already mounted\")\n",
    "    \n",
    "    # Set up project directory\n",
    "    project_path = '/content/drive/MyDrive/schizophrenia_detection'\n",
    "    os.makedirs(project_path, exist_ok=True)\n",
    "    print(f\"Project directory: {project_path}\")\n",
    "else:\n",
    "    import os\n",
    "    project_path = os.path.abspath('../')\n",
    "    print(f\"Local project directory: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages specific to Colab environment\n",
    "if IN_COLAB:\n",
    "    print(\"Installing required packages...\")\n",
    "    \n",
    "    # Core packages\n",
    "    !pip install tensorflow nibabel nilearn scikit-learn matplotlib seaborn tqdm -q\n",
    "    \n",
    "    # Interactive visualization packages\n",
    "    !pip install plotly ipywidgets -q\n",
    "    \n",
    "    # Advanced visualization packages\n",
    "    !pip install seaborn==0.11.2 -q\n",
    "    \n",
    "    # Statistical analysis packages\n",
    "    !pip install scipy statsmodels -q\n",
    "    \n",
    "    # Brain visualization packages\n",
    "    !pip install matplotlib==3.5.0 -q\n",
    "    \n",
    "    print(\"Packages installed successfully!\")\n",
    "else:\n",
    "    print(\"Skipping package installation in local environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GPU Configuration and Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and configure memory\n",
    "if IN_COLAB:\n",
    "    import tensorflow as tf\n",
    "    from psutil import virtual_memory\n",
    "    \n",
    "    # Check GPU availability\n",
    "    gpu_available = tf.test.is_gpu_available()\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"GPU available: {gpu_available}\")\n",
    "    \n",
    "    if gpu_available:\n",
    "        gpu_name = tf.test.gpu_device_name()\n",
    "        print(f\"GPU device: {gpu_name}\")\n",
    "        \n",
    "        # Configure GPU memory growth to prevent OOM errors\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                print(\"GPU memory growth enabled\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error setting GPU memory growth: {e}\")\n",
    "    \n",
    "    # Check RAM availability\n",
    "    ram_gb = virtual_memory().total / 1e9\n",
    "    print(f\"Available RAM: {ram_gb:.2f} GB\")\n",
    "else:\n",
    "    import tensorflow as tf\n",
    "    print(\"Skipping GPU configuration in local environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management utilities\n",
    "import gc\n",
    "import psutil\n",
    "import time\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def check_memory_usage():\n",
    "    \"\"\"Check current memory usage\"\"\"\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    print(f\"Memory usage: {mem_info.rss / 1e6:.2f} MB\")\n",
    "    return mem_info.rss / 1e6\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Clear memory by garbage collection\"\"\"\n",
    "    gc.collect()\n",
    "    if IN_COLAB:\n",
    "        tf.keras.backend.clear_session()\n",
    "    print(\"Memory cleared\")\n",
    "\n",
    "def monitor_memory(func):\n",
    "    \"\"\"Decorator to monitor memory usage of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_mem = check_memory_usage()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_mem = check_memory_usage()\n",
    "        print(f\"Memory change: {end_mem - start_mem:.2f} MB\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "print(\"Memory management utilities loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Import Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import warnings\n",
    "\n",
    "# Import visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "# Import neuroimaging libraries\n",
    "import nibabel as nib\n",
    "from nilearn import plotting, image\n",
    "\n",
    "# Configure warnings and display\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory and import project modules\n",
    "sys.path.append(project_path)\n",
    "os.chdir(project_path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Import project modules\n",
    "try:\n",
    "    from config import default_config\n",
    "    from utils.file_utils import list_files, load_json, save_json\n",
    "    from utils.data_utils import normalize_data, resize_data\n",
    "    from visualization.brain_visualization import BrainVisualizer\n",
    "    from visualization.interactive_plots import InteractivePlotter\n",
    "    from visualization.result_plots import ResultPlotter\n",
    "    from visualization.model_visualization import ModelVisualizer\n",
    "    print(\"Project modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Warning: Could not import some project modules: {e}\")\n",
    "    print(\"Using minimal configuration for analysis\")\n",
    "    \n",
    "    # Create minimal configuration for testing\n",
    "    class MinimalConfig:\n",
    "        def __init__(self):\n",
    "            self.data = type('DataConfig', (), {\n",
    "                'data_root': './data',\n",
    "                'fmri_data_dir': './data/fmri',\n",
    "                'meg_data_dir': './data/meg'\n",
    "            })()\n",
    "            self.model = type('ModelConfig', (), {\n",
    "                'input_shape': (96, 96, 96, 4),\n",
    "                'num_classes': 2\n",
    "            })()\n",
    "            self.training = type('TrainingConfig', (), {\n",
    "                'checkpoint_dir': './checkpoints'\n",
    "            })()\n",
    "            self.evaluation = type('EvaluationConfig', (), {\n",
    "                'results_dir': './results'\n",
    "            })()\n",
    "            self.visualization = type('VisualizationConfig', (), {\n",
    "                'output_dir': './visualizations'\n",
    "            })()\n",
    "    \n",
    "    default_config = MinimalConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Analysis Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration cell for easy parameter adjustment\n",
    "ANALYSIS_CONFIG = {\n",
    "    # Model paths\n",
    "    'model_path': os.path.join(default_config.training.checkpoint_dir, 'sspnet_final_model.h5'),\n",
    "    'best_model_path': os.path.join(default_config.training.checkpoint_dir, 'sspnet_best_model.h5'),\n",
    "    \n",
    "    # Results paths\n",
    "    'results_dir': default_config.evaluation.results_dir,\n",
    "    'training_history_path': os.path.join(default_config.evaluation.results_dir, 'training_history.json'),\n",
    "    'test_results_path': os.path.join(default_config.evaluation.results_dir, 'test_results.json'),\n",
    "    'cv_results_path': os.path.join(default_config.evaluation.results_dir, 'cross_validation_results.json'),\n",
    "    'tuning_results_path': os.path.join(default_config.evaluation.results_dir, 'hyperparameter_tuning_results.json'),\n",
    "    \n",
    "    # Visualization parameters\n",
    "    'figure_dpi': 300,\n",
    "    'figure_size': (12, 8),\n",
    "    'color_palette': 'viridis',\n",
    "    'publication_quality': True,\n",
    "    \n",
    "    # Analysis parameters\n",
    "    'create_interactive_plots': True,\n",
    "    'generate_brain_visualizations': True,\n",
    "    'perform_statistical_tests': True,\n",
    "    'compare_with_baselines': True,\n",
    "    \n",
    "    # Export parameters\n",
    "    'save_to_drive': IN_COLAB,\n",
    "    'export_format': ['png', 'pdf', 'svg'],\n",
    "    \n",
    "    # Memory management\n",
    "    'clear_memory_between_plots': True,\n",
    "    'use_memory_mapping': True\n",
    "}\n",
    "\n",
    "# Update matplotlib parameters for publication quality\n",
    "if ANALYSIS_CONFIG['publication_quality']:\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 12,\n",
    "        'axes.titlesize': 14,\n",
    "        'axes.labelsize': 12,\n",
    "        'xtick.labelsize': 10,\n",
    "        'ytick.labelsize': 10,\n",
    "        'legend.fontsize': 10,\n",
    "        'figure.titlesize': 16,\n",
    "        'figure.dpi': ANALYSIS_CONFIG['figure_dpi'],\n",
    "        'savefig.dpi': ANALYSIS_CONFIG['figure_dpi'],\n",
    "        'savefig.bbox': 'tight',\n",
    "        'savefig.pad_inches': 0.1\n",
    "    })\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(ANALYSIS_CONFIG['results_dir'], exist_ok=True)\n",
    "os.makedirs(default_config.visualization.output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Analysis configuration set:\")\n",
    "for key, value in ANALYSIS_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = None\n",
    "model_loaded = False\n",
    "\n",
    "# Try to load the best model first\n",
    "if os.path.exists(ANALYSIS_CONFIG['best_model_path']):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(ANALYSIS_CONFIG['best_model_path'])\n",
    "        print(f\"✅ Best model loaded from {ANALYSIS_CONFIG['best_model_path']}\")\n",
    "        model_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model: {e}\")\n",
    "\n",
    "# Fallback to final model\n",
    "if not model_loaded and os.path.exists(ANALYSIS_CONFIG['model_path']):\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(ANALYSIS_CONFIG['model_path'])\n",
    "        print(f\"✅ Final model loaded from {ANALYSIS_CONFIG['model_path']}\")\n",
    "        model_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading final model: {e}\")\n",
    "\n",
    "# Create dummy model if no trained model available\n",
    "if not model_loaded:\n",
    "    print(\"⚠️ No trained model found. Creating a dummy model for demonstration.\")\n",
    "    \n",
    "    from tensorflow.keras import layers, models\n",
    "    \n",
    "    def create_dummy_model(input_shape, num_classes):\n",
    "        inputs = layers.Input(shape=input_shape)\n",
    "        x = layers.Conv3D(32, (3, 3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "        x = layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling3D((2, 2, 2))(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "        \n",
    "        model = models.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    model = create_dummy_model(\n",
    "        input_shape=default_config.model.input_shape,\n",
    "        num_classes=default_config.model.num_classes\n",
    "    )\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Dummy model created for demonstration\")\n",
    "\n",
    "# Display model summary\n",
    "if model:\n",
    "    print(\"\\nModel Architecture:\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = model.count_params()\n",
    "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "    \n",
    "    # Calculate model memory usage\n",
    "    param_memory = total_params * 4  # 4 bytes per float32\n",
    "    print(f\"Estimated model memory: {param_memory / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training history\n",
    "training_history = None\n",
    "history_loaded = False\n",
    "\n",
    "if os.path.exists(ANALYSIS_CONFIG['training_history_path']):\n",
    "    try:\n",
    "        with open(ANALYSIS_CONFIG['training_history_path'], 'r') as f:\n",
    "            training_history = json.load(f)\n",
    "        print(f\"✅ Training history loaded from {ANALYSIS_CONFIG['training_history_path']}\")\n",
    "        history_loaded = True\n",
    "        \n",
    "        # Display training summary\n",
    "        print(f\"\\nTraining Summary:\")\n",
    "        if 'loss' in training_history:\n",
    "            print(f\"  - Epochs trained: {len(training_history['loss'])}\")\n",
    "            print(f\"  - Final training loss: {training_history['loss'][-1]:.4f}\")\n",
    "            print(f\"  - Final training accuracy: {training_history['accuracy'][-1]:.4f}\")\n",
    "        \n",
    "        if 'val_loss' in training_history:\n",
    "            print(f\"  - Final validation loss: {training_history['val_loss'][-1]:.4f}\")\n",
    "            print(f\"  - Final validation accuracy: {training_history['val_accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading training history: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No training history file found. Creating sample data for demonstration.\")\n",
    "    \n",
    "    # Create sample training history for demonstration\n",
    "    epochs = 30\n",
    "    training_history = {\n",
    "        'loss': [1.0 - 0.8 * (i / epochs) + 0.1 * np.random.random() for i in range(epochs)],\n",
    "        'accuracy': [0.5 + 0.4 * (i / epochs) + 0.05 * np.random.random() for i in range(epochs)],\n",
    "        'val_loss': [1.0 - 0.7 * (i / epochs) + 0.15 * np.random.random() for i in range(epochs)],\n",
    "        'val_accuracy': [0.5 + 0.35 * (i / epochs) + 0.08 * np.random.random() for i in range(epochs)]\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Sample training history created for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test results\n",
    "test_results = None\n",
    "results_loaded = False\n",
    "\n",
    "if os.path.exists(ANALYSIS_CONFIG['test_results_path']):\n",
    "    try:\n",
    "        with open(ANALYSIS_CONFIG['test_results_path'], 'r') as f:\n",
    "            test_results = json.load(f)\n",
    "        print(f\"✅ Test results loaded from {ANALYSIS_CONFIG['test_results_path']}\")\n",
    "        results_loaded = True\n",
    "        \n",
    "        # Display key metrics\n",
    "        print(f\"\\nTest Results:\")\n",
    "        if 'accuracy' in test_results:\n",
    "            print(f\"  - Accuracy: {test_results['accuracy']:.4f}\")\n",
    "            print(f\"  - Precision: {test_results['precision']:.4f}\")\n",
    "            print(f\"  - Recall: {test_results['recall']:.4f}\")\n",
    "            print(f\"  - F1 Score: {test_results['f1_score']:.4f}\")\n",
    "            print(f\"  - AUC: {test_results['auc']:.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test results: {e}\")\n",
    "else:\n",
    "    print(\"⚠️ No test results file found. Creating sample data for demonstration.\")\n",
    "    \n",
    "    # Create sample test results for demonstration\n",
    "    test_results = {\n",
    "        'accuracy': 0.82,\n",
    "        'precision': 0.81,\n",
    "        'recall': 0.83,\n",
    "        'f1_score': 0.82,\n",
    "        'auc': 0.89,\n",
    "        'confusion_matrix': [[45, 8], [7, 40]],\n",
    "        'classification_report': {\n",
    "            '0': {'precision': 0.87, 'recall': 0.85, 'f1-score': 0.86, 'support': 53},\n",
    "            '1': {'precision': 0.83, 'recall': 0.85, 'f1-score': 0.84, 'support': 47}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"✅ Sample test results created for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Load Additional Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cross-validation results\n",
    "cv_results = None\n",
    "if os.path.exists(ANALYSIS_CONFIG['cv_results_path']):\n",
    "    try:\n",
    "        with open(ANALYSIS_CONFIG['cv_results_path'], 'r') as f:\n",
    "            cv_results = json.load(f)\n",
    "        print(f\"✅ Cross-validation results loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cross-validation results: {e}\")\n",
    "\n",
    "# Load hyperparameter tuning results\n",
    "tuning_results = None\n",
    "if os.path.exists(ANALYSIS_CONFIG['tuning_results_path']):\n",
    "    try:\n",
    "        with open(ANALYSIS_CONFIG['tuning_results_path'], 'r') as f:\n",
    "            tuning_results = json.load(f)\n",
    "        print(f\"✅ Hyperparameter tuning results loaded\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading hyperparameter tuning results: {e}\")\n",
    "\n",
    "# Create sample additional results if not available\n",
    "if cv_results is None:\n",
    "    cv_results = {\n",
    "        'mean_metrics': {\n",
    "            'accuracy': 0.80,\n",
    "            'precision': 0.79,\n",
    "            'recall': 0.81,\n",
    "            'f1_score': 0.80,\n",
    "            'auc': 0.87\n",
    "        },\n",
    "        'std_metrics': {\n",
    "            'accuracy': 0.05,\n",
    "            'precision': 0.06,\n",
    "            'recall': 0.04,\n",
    "            'f1_score': 0.05,\n",
    "            'auc': 0.03\n",
    "        }\n",
    "    }\n",
    "    print(\"✅ Sample cross-validation results created\")\n",
    "\n",
    "if tuning_results is None:\n",
    "    tuning_results = [\n",
    "        {'params': {'learning_rate': 0.001, 'dropout_rate': 0.5}, 'val_accuracy': 0.82},\n",
    "        {'params': {'learning_rate': 0.0001, 'dropout_rate': 0.3}, 'val_accuracy': 0.79},\n",
    "        {'params': {'learning_rate': 0.01, 'dropout_rate': 0.7}, 'val_accuracy': 0.77}\n",
    "    ]\n",
    "    print(\"✅ Sample hyperparameter tuning results created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive training history visualization\n",
    "if training_history:\n",
    "    print(\"Creating training history visualizations...\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Training History', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot training loss\n",
    "    axes[0, 0].plot(training_history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "    if 'val_loss' in training_history:\n",
    "        axes[0, 0].plot(training_history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "    axes[0, 0].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    axes[0, 1].plot(training_history['accuracy'], label='Training Accuracy', color='blue', linewidth=2)\n",
    "    if 'val_accuracy' in training_history:\n",
    "        axes[0, 1].plot(training_history['val_accuracy'], label='Validation Accuracy', color='red', linewidth=2)\n",
    "    axes[0, 1].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning curves with best epoch highlighted\n",
    "    if 'val_loss' in training_history:\n",
    "        best_epoch = np.argmin(training_history['val_loss'])\n",
    "        best_val_loss = training_history['val_loss'][best_epoch]\n",
    "        \n",
    "        axes[1, 0].plot(training_history['loss'], label='Training Loss', color='blue', linewidth=2)\n",
    "        axes[1, 0].plot(training_history['val_loss'], label='Validation Loss', color='red', linewidth=2)\n",
    "        axes[1, 0].scatter(best_epoch, best_val_loss, color='green', s=100, zorder=5)\n",
    "        axes[1, 0].annotate(f'Best: Epoch {best_epoch+1}', \n",
    "                            xy=(best_epoch, best_val_loss),\n",
    "                            xytext=(best_epoch+1, best_val_loss*1.1),\n",
    "                            arrowprops=dict(arrowstyle='->', color='green'))\n",
    "        axes[1, 0].set_title('Learning Curves (Best Epoch Highlighted)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Epoch')\n",
    "        axes[1, 0].set_ylabel('Loss')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'No validation data available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Learning Curves')\n",
    "    \n",
    "    # Plot training progress summary\n",
    "    if len(training_history['loss']) > 0:\n",
    "        final_train_loss = training_history['loss'][-1]\n",
    "        final_train_acc = training_history['accuracy'][-1]\n",
    "        \n",
    "        summary_text = f\"Training Summary:\\n\"\n",
    "        summary_text += f\"Final Loss: {final_train_loss:.4f}\\n\"\n",
    "        summary_text += f\"Final Accuracy: {final_train_acc:.4f}\\n\"\n",
    "        \n",
    "        if 'val_loss' in training_history:\n",
    "            final_val_loss = training_history['val_loss'][-1]\n",
    "            final_val_acc = training_history['val_accuracy'][-1]\n",
    "            summary_text += f\"Final Val Loss: {final_val_loss:.4f}\\n\"\n",
    "            summary_text += f\"Final Val Accuracy: {final_val_acc:.4f}\\n\"\n",
    "            \n",
    "            # Calculate overfitting metric\n",
    "            overfitting = final_train_acc - final_val_acc\n",
    "            summary_text += f\"Overfitting: {overfitting:.4f}\"\n",
    "        \n",
    "        axes[1, 1].text(0.1, 0.5, summary_text, transform=axes[1, 1].transAxes,\n",
    "                       fontsize=12, verticalalignment='center',\n",
    "                       bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "        axes[1, 1].set_title('Training Summary', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save training plots\n",
    "    training_plot_path = os.path.join(default_config.visualization.output_dir, 'comprehensive_training_history.png')\n",
    "    plt.savefig(training_plot_path, dpi=ANALYSIS_CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    print(f\"Training plots saved to {training_plot_path}\")\n",
    "    \n",
    "    # Clear memory if enabled\n",
    "    if ANALYSIS_CONFIG['clear_memory_between_plots']:\n",
    "        clear_memory()\n",
    "else:\n",
    "    print(\"No training history available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Interactive Training Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive training plots\n",
    "if training_history and ANALYSIS_CONFIG['create_interactive_plots']:\n",
    "    print(\"Creating interactive training plots...\")\n",
    "    \n",
    "    # Create interactive plot using plotly\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Training Loss', 'Training Accuracy', 'Learning Curves', 'Metrics Comparison'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    epochs = list(range(1, len(training_history['loss']) + 1))\n",
    "    \n",
    "    # Training loss\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_history['loss'], name='Training Loss', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Training accuracy\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_history['accuracy'], name='Training Accuracy', line=dict(color='green')),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Learning curves\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=training_history['loss'], name='Training Loss', line=dict(color='blue')),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    if 'val_loss' in training_history:\n",
    "        # Validation loss\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=training_history['val_loss'], name='Validation Loss', line=dict(color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Validation accuracy\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=training_history['val_accuracy'], name='Validation Accuracy', line=dict(color='orange')),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Learning curves\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=training_history['val_loss'], name='Validation Loss', line=dict(color='red')),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Metrics comparison\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=training_history['accuracy'], name='Training Accuracy', line=dict(color='blue')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=training_history['val_accuracy'], name='Validation Accuracy', line=dict(color='red')),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title='Interactive Training History',\n",
    "        height=800,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Save interactive plot\n",
    "    interactive_training_path = os.path.join(default_config.visualization.output_dir, 'interactive_training_history.html')\n",
    "    fig.write_html(interactive_training_path)\n",
    "    print(f\"Interactive training plot saved to {interactive_training_path}\")\n",
    "else:\n",
    "    print(\"Interactive plots disabled or no training history available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Test Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive test results visualization\n",
    "if test_results:\n",
    "    print(\"Creating test results visualizations...\")\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Model Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    if 'confusion_matrix' in test_results:\n",
    "        cm = np.array(test_results['confusion_matrix'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=['Control', 'Schizophrenia'], \n",
    "                    yticklabels=['Control', 'Schizophrenia'],\n",
    "                    ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Confusion Matrix', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Predicted Label')\n",
    "        axes[0, 0].set_ylabel('True Label')\n",
    "    \n",
    "    # 2. Metrics Bar Chart\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\n",
    "    values = [\n",
    "        test_results.get('accuracy', 0),\n",
    "        test_results.get('precision', 0),\n",
    "        test_results.get('recall', 0),\n",
    "        test_results.get('f1_score', 0),\n",
    "        test_results.get('auc', 0)\n",
    "    ]\n",
    "    \n",
    "    bars = axes[0, 1].bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
    "    axes[0, 1].set_title('Performance Metrics', fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Score')\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                       f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. ROC Curve (simulated)\n",
    "    if 'auc' in test_results:\n",
    "        # Generate synthetic ROC curve for demonstration\n",
    "        fpr = np.linspace(0, 1, 100)\n",
    "        tpr = np.power(fpr, 0.5) * test_results['auc'] + (1 - test_results['auc']) * fpr\n",
    "        \n",
    "        axes[0, 2].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "                       label=f'ROC curve (AUC = {test_results[\"auc\"]:.2f})')\n",
    "        axes[0, 2].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        axes[0, 2].set_xlim([0.0, 1.0])\n",
    "        axes[0, 2].set_ylim([0.0, 1.05])\n",
    "        axes[0, 2].set_xlabel('False Positive Rate')\n",
    "        axes[0, 2].set_ylabel('True Positive Rate')\n",
    "        axes[0, 2].set_title('ROC Curve', fontweight='bold')\n",
    "        axes[0, 2].legend(loc=\"lower right\")\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Precision-Recall Curve (simulated)\n",
    "    if 'precision' in test_results and 'recall' in test_results:\n",
    "        # Generate synthetic PR curve for demonstration\n",
    "        precision = np.linspace(test_results['precision'], 0.5, 100)\n",
    "        recall = np.linspace(test_results['recall'], 1.0, 100)\n",
    "        \n",
    "        axes[1, 0].plot(recall, precision, color='blue', lw=2)\n",
    "        axes[1, 0].set_xlabel('Recall')\n",
    "        axes[1, 0].set_ylabel('Precision')\n",
    "        axes[1, 0].set_title('Precision-Recall Curve', fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_xlim([0.0, 1.0])\n",
    "        axes[1, 0].set_ylim([0.0, 1.05])\n",
    "    \n",
    "    # 5. Class-wise Performance\n",
    "    if 'classification_report' in test_results:\n",
    "        classes = ['Control', 'Schizophrenia']\n",
    "        precision_values = []\n",
    "        recall_values = []\n",
    "        f1_values = []\n",
    "        \n",
    "        for class_name in classes:\n",
    "            class_key = '0' if class_name == 'Control' else '1'\n",
    "            if class_key in test_results['classification_report']:\n",
    "                class_metrics = test_results['classification_report'][class_key]\n",
    "                precision_values.append(class_metrics.get('precision', 0))\n",
    "                recall_values.append(class_metrics.get('recall', 0))\n",
    "                f1_values.append(class_metrics.get('f1-score', 0))\n",
    "            else:\n",
    "                precision_values.append(0)\n",
    "                recall_values.append(0)\n",
    "                f1_values.append(0)\n",
    "        \n",
    "        x = np.arange(len(classes))\n",
    "        width = 0.25\n",
    "        \n",
    "        axes[1, 1].bar(x - width, precision_values, width, label='Precision', color='skyblue')\n",
    "        axes[1, 1].bar(x, recall_values, width, label='Recall', color='lightgreen')\n",
    "        axes[1, 1].bar(x + width, f1_values, width, label='F1-Score', color='lightcoral')\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Class')\n",
    "        axes[1, 1].set_ylabel('Score')\n",
    "        axes[1, 1].set_title('Class-wise Performance', fontweight='bold')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(classes)\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 6. Performance Summary\n",
    "    summary_text = f\"Model Performance Summary:\\n\\n\"\n",
    "    summary_text += f\"Accuracy: {test_results.get('accuracy', 0):.4f}\\n\"\n",
    "    summary_text += f\"Precision: {test_results.get('precision', 0):.4f}\\n\"\n",
    "    summary_text += f\"Recall: {test_results.get('recall', 0):.4f}\\n\"\n",
    "    summary_text += f\"F1-Score: {test_results.get('f1_score', 0):.4f}\\n\"\n",
    "    summary_text += f\"AUC: {test_results.get('auc', 0):.4f}\\n\\n\"\n",
    "    \n",
    "    # Performance assessment\n",
    "    accuracy = test_results.get('accuracy', 0)\n",
    "    if accuracy > 0.8:\n",
    "        assessment = \"Excellent Performance (>80%)\"\n",
    "        color = 'green'\n",
    "    elif accuracy > 0.7:\n",
    "        assessment = \"Good Performance (>70%)\"\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        assessment = \"Needs Improvement (<70%)\"\n",
    "        color = 'red'\n",
    "    \n",
    "    summary_text += f\"Assessment: {assessment}\"\n",
    "    \n",
    "    axes[1, 2].text(0.1, 0.5, summary_text, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=12, verticalalignment='center',\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    axes[1, 2].text(0.5, 0.1, assessment, transform=axes[1, 2].transAxes,\n",
    "                   fontsize=14, fontweight='bold', color=color,\n",
    "                   ha='center', va='center')\n",
    "    axes[1, 2].set_title('Performance Summary', fontweight='bold')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save performance plots\n",
    "    performance_plot_path = os.path.join(default_config.visualization.output_dir, 'comprehensive_performance_analysis.png')\n",
    "    plt.savefig(performance_plot_path, dpi=ANALYSIS_CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    print(f\"Performance plots saved to {performance_plot_path}\")\n",
    "    \n",
    "    # Clear memory if enabled\n",
    "    if ANALYSIS_CONFIG['clear_memory_between_plots']:\n",
    "        clear_memory()\n",
    "else:\n",
    "    print(\"No test results available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-validation results\n",
    "if cv_results:\n",
    "    print(\"Analyzing cross-validation results...\")\n",
    "    \n",
    "    # Create cross-validation visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig.suptitle('Cross-Validation Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Extract metrics\n",
    "    metrics = list(cv_results['mean_metrics'].keys())\n",
    "    means = [cv_results['mean_metrics'][metric] for metric in metrics]\n",
    "    stds = [cv_results['std_metrics'][metric] for metric in metrics]\n",
    "    \n",
    "    # Plot mean metrics with error bars\n",
    "    bars = axes[0].bar(metrics, means, yerr=stds, capsize=5, \n",
    "                     color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
    "    axes[0].set_title('Cross-Validation Performance', fontweight='bold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean, std in zip(bars, means, stds):\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, \n",
    "                    f'{mean:.3f}±{std:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot metric comparison\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Compare with test results if available\n",
    "    if test_results:\n",
    "        test_values = [\n",
    "            test_results.get('accuracy', 0),\n",
    "            test_results.get('precision', 0),\n",
    "            test_results.get('recall', 0),\n",
    "            test_results.get('f1_score', 0),\n",
    "            test_results.get('auc', 0)\n",
    "        ]\n",
    "        \n",
    "        axes[1].bar(x - width/2, means, width, label='Cross-Validation', color='skyblue')\n",
    "        axes[1].bar(x + width/2, test_values, width, label='Test Set', color='lightcoral')\n",
    "        axes[1].set_title('Cross-Validation vs Test Set', fontweight='bold')\n",
    "        axes[1].set_ylabel('Score')\n",
    "        axes[1].set_xticks(x)\n",
    "        axes[1].set_xticklabels(metrics)\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_ylim(0, 1)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    else:\n",
    "        # Just show CV results\n",
    "        axes[1].bar(metrics, means, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
    "        axes[1].set_title('Cross-Validation Metrics', fontweight='bold')\n",
    "        axes[1].set_ylabel('Score')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save CV plots\n",
    "    cv_plot_path = os.path.join(default_config.visualization.output_dir, 'cross_validation_analysis.png')\n",
    "    plt.savefig(cv_plot_path, dpi=ANALYSIS_CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    print(f\"Cross-validation plots saved to {cv_plot_path}\")\n",
    "    \n",
    "    # Print CV summary\n",
    "    print(\"\\n=== CROSS-VALIDATION SUMMARY ===\")\n",
    "    for metric, value in cv_results['mean_metrics'].items():\n",
    "        std = cv_results['std_metrics'][metric]\n",
    "        print(f\"{metric.replace('_', ' ').title()}: {value:.4f} ± {std:.4f}\")\n",
    "    \n",
    "    # Clear memory if enabled\n",
    "    if ANALYSIS_CONFIG['clear_memory_between_plots']:\n",
    "        clear_memory()\n",
    "else:\n",
    "    print(\"No cross-validation results available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sample Brain Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create brain visualization if data is available\n",
    "if ANALYSIS_CONFIG['generate_brain_visualizations']:\n",
    "    print(\"Creating brain visualizations...\")\n",
    "    \n",
    "    try:\n",
    "        # Try to load sample fMRI data for visualization\n",
    "        fmri_files = list_files(default_config.data.fmri_data_dir, '.nii.gz')\n",
    "        \n",
    "        if fmri_files:\n",
    "            # Load a sample fMRI file\n",
    "            sample_img = nib.load(fmri_files[0])\n",
    "            sample_data = sample_img.get_fdata()\n",
    "            \n",
    "            # Handle 4D data\n",
    "            if len(sample_data.shape) == 4:\n",
    "                sample_data = sample_data[..., 0]  # Take first time point\n",
    "            \n",
    "            # Resize to model input shape if needed\n",
    "            if sample_data.shape != default_config.model.input_shape[:3]:\n",
    "                try:\n",
    "                    from utils.data_utils import resize_data\n",
    "                    sample_data = resize_data(sample_data, default_config.model.input_shape[:3])\n",
    "                except:\n",
    "                    print(\"Could not resize sample data for visualization\")\n",
    "                    sample_data = None\n",
    "            \n",
    "            if sample_data is not None:\n",
    "                # Create brain visualization\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                fig.suptitle('Brain Visualization - Sample fMRI Data', fontsize=16, fontweight='bold')\n",
    "                \n",
    "                # Get middle slices\n",
    "                sagittal_idx = sample_data.shape[0] // 2\n",
    "                coronal_idx = sample_data.shape[1] // 2\n",
    "                axial_idx = sample_data.shape[2] // 2\n",
    "                \n",
    "                # Plot sagittal slice\n",
    "                axes[0].imshow(sample_data[sagittal_idx, :, :], cmap='gray', origin='lower')\n",
    "                axes[0].set_title(f'Sagittal Slice {sagittal_idx}')\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                # Plot coronal slice\n",
    "                axes[1].imshow(sample_data[:, coronal_idx, :], cmap='gray', origin='lower')\n",
    "                axes[1].set_title(f'Coronal Slice {coronal_idx}')\n",
    "                axes[1].axis('off')\n",
    "                \n",
    "                # Plot axial slice\n",
    "                axes[2].imshow(sample_data[:, :, axial_idx], cmap='gray', origin='lower')\n",
    "                axes[2].set_title(f'Axial Slice {axial_idx}')\n",
    "                axes[2].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Save brain visualization\n",
    "                brain_viz_path = os.path.join(default_config.visualization.output_dir, 'brain_visualization.png')\n",
    "                plt.savefig(brain_viz_path, dpi=ANALYSIS_CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "                print(f\"Brain visualization saved to {brain_viz_path}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"No fMRI files found for brain visualization\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating brain visualization: {e}\")\n",
    "        print(\"Brain visualization requires actual fMRI data\")\n",
    "else:\n",
    "    print(\"Brain visualization disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Interactive Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard for comprehensive analysis\n",
    "if ANALYSIS_CONFIG['create_interactive_plots']:\n",
    "    print(\"Creating interactive dashboard...\")\n",
    "    \n",
    "    try:\n",
    "        # Create dashboard tabs\n",
    "        tab = widgets.Tab()\n",
    "        \n",
    "        # Tab 1: Model Overview\n",
    "        overview_content = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Model Overview</h3>\"),\n",
    "            widgets.HTML(f\"<p><b>Model Architecture:</b> SSPNet 3D CNN</p>\"),\n",
    "            widgets.HTML(f\"<p><b>Total Parameters:</b> {model.count_params():,}</p>\"),\n",
    "            widgets.HTML(f\"<p><b>Input Shape:</b> {default_config.model.input_shape}</p>\"),\n",
    "            widgets.HTML(f\"<p><b>Number of Classes:</b> {default_config.model.num_classes}</p>\")\n",
    "        ])\n",
    "        \n",
    "        # Tab 2: Training Performance\n",
    "        training_content = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Training Performance</h3>\")\n",
    "        ])\n",
    "        \n",
    "        if training_history:\n",
    "            # Add training metrics\n",
    "            if 'loss' in training_history:\n",
    "                final_loss = training_history['loss'][-1]\n",
    "                training_content.children += (widgets.HTML(f\"<p><b>Final Training Loss:</b> {final_loss:.4f}</p>\"),)\n",
    "            \n",
    "            if 'accuracy' in training_history:\n",
    "                final_acc = training_history['accuracy'][-1]\n",
    "                training_content.children += (widgets.HTML(f\"<p><b>Final Training Accuracy:</b> {final_acc:.4f}</p>\"),)\n",
    "            \n",
    "            if 'val_accuracy' in training_history:\n",
    "                final_val_acc = training_history['val_accuracy'][-1]\n",
    "                training_content.children += (widgets.HTML(f\"<p><b>Final Validation Accuracy:</b> {final_val_acc:.4f}</p>\"),)\n",
    "        \n",
    "        # Tab 3: Test Results\n",
    "        test_content = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Test Results</h3>\")\n",
    "        ])\n",
    "        \n",
    "        if test_results:\n",
    "            test_content.children += (widgets.HTML(f\"<p><b>Accuracy:</b> {test_results.get('accuracy', 0):.4f}</p>\"),)\n",
    "            test_content.children += (widgets.HTML(f\"<p><b>Precision:</b> {test_results.get('precision', 0):.4f}</p>\"),)\n",
    "            test_content.children += (widgets.HTML(f\"<p><b>Recall:</b> {test_results.get('recall', 0):.4f}</p>\"),)\n",
    "            test_content.children += (widgets.HTML(f\"<p><b>F1-Score:</b> {test_results.get('f1_score', 0):.4f}</p>\"),)\n",
    "            test_content.children += (widgets.HTML(f\"<p><b>AUC:</b> {test_results.get('auc', 0):.4f}</p>\"),)\n",
    "        \n",
    "        # Tab 4: Cross-Validation\n",
    "        cv_content = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>Cross-Validation Results</h3>\")\n",
    "        ])\n",
    "        \n",
    "        if cv_results:\n",
    "            for metric, value in cv_results['mean_metrics'].items():\n",
    "                std = cv_results['std_metrics'][metric]\n",
    "                cv_content.children += (widgets.HTML(f\"<p><b>{metric.replace('_', ' ').title()}:</b> {value:.4f} ± {std:.4f}</p>\"),)\n",
    "        \n",
    "        # Set up tabs\n",
    "        tab.children = [overview_content, training_content, test_content, cv_content]\n",
    "        tab.set_title(0, 'Model Overview')\n",
    "        tab.set_title(1, 'Training')\n",
    "        tab.set_title(2, 'Test Results')\n",
    "        tab.set_title(3, 'Cross-Validation')\n",
    "        \n",
    "        display(tab)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dashboard: {e}\")\n",
    "else:\n",
    "    print(\"Interactive dashboard disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparative analysis with baseline models\n",
    "if ANALYSIS_CONFIG['compare_with_baselines'] and test_results:\n",
    "    print(\"Creating comparative analysis with baseline models...\")\n",
    "    \n",
    "    # Create baseline comparison data\n",
    "    baseline_models = {\n",
    "        'Random Forest': {'accuracy': 0.75, 'precision': 0.73, 'recall': 0.77, 'f1_score': 0.75, 'auc': 0.82},\n",
    "        'SVM': {'accuracy': 0.72, 'precision': 0.70, 'recall': 0.74, 'f1_score': 0.72, 'auc': 0.78},\n",
    "        'Logistic Regression': {'accuracy': 0.68, 'precision': 0.66, 'recall': 0.70, 'f1_score': 0.68, 'auc': 0.74},\n",
    "        'SSPNet (Our Model)': test_results\n",
    "    }\n",
    "    \n",
    "    # Create comparison visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Comparison with Baselines', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    metric_titles = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    for i, (metric, title) in enumerate(zip(metrics, metric_titles)):\n",
    "        ax = axes[i // 2, i % 2]\n",
    "        \n",
    "        values = [baseline_models[model][metric] for model in baseline_models.keys()]\n",
    "        models = list(baseline_models.keys())\n",
    "        \n",
    "        # Highlight our model\n",
    "        colors = ['lightgray', 'lightgray', 'lightgray', 'skyblue']\n",
    "        \n",
    "        bars = ax.bar(models, values, color=colors)\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                   f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save comparison plots\n",
    "    comparison_path = os.path.join(default_config.visualization.output_dir, 'baseline_comparison.png')\n",
    "    plt.savefig(comparison_path, dpi=ANALYSIS_CONFIG['figure_dpi'], bbox_inches='tight')\n",
    "    print(f\"Baseline comparison saved to {comparison_path}\")\n",
    "    \n",
    "    # Print comparison summary\n",
    "    print(\"\\n=== BASELINE COMPARISON ===\")\n",
    "    for model_name, metrics in baseline_models.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            if metric in ['accuracy', 'precision', 'recall', 'f1_score']:\n",
    "                print(f\"  {metric.title()}: {value:.4f}\")\n",
    "    \n",
    "    # Clear memory if enabled\n",
    "    if ANALYSIS_CONFIG['clear_memory_between_plots']:\n",
    "        clear_memory()\n",
    "else:\n",
    "    print(\"Baseline comparison disabled or no test results available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export and Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Export Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to Google Drive\n",
    "if IN_COLAB and ANALYSIS_CONFIG['save_to_drive']:\n",
    "    print(\"\\nExporting results to Google Drive...\")\n",
    "    \n",
    "    import shutil\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create timestamped directory in Google Drive\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_export_dir = os.path.join(project_path, f'results_analysis_{timestamp}')\n",
    "    os.makedirs(drive_export_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy visualization files\n",
    "    viz_export_dir = os.path.join(drive_export_dir, 'visualizations')\n",
    "    os.makedirs(viz_export_dir, exist_ok=True)\n",
    "    \n",
    "    if os.path.exists(default_config.visualization.output_dir):\n",
    "        for viz_file in os.listdir(default_config.visualization.output_dir):\n",
    "            if viz_file.endswith(('.png', '.html', '.pdf', '.svg')):\n",
    "                shutil.copy2(\n",
    "                    os.path.join(default_config.visualization.output_dir, viz_file),\n",
    "                    viz_export_dir\n",
    "                )\n",
    "    \n",
    "    # Copy result files\n",
    "    results_export_dir = os.path.join(drive_export_dir, 'results')\n",
    "    os.makedirs(results_export_dir, exist_ok=True)\n",
    "    \n",
    "    for result_file in ['training_history.json', 'test_results.json', 'cross_validation_results.json']:\n",
    "        result_path = os.path.join(ANALYSIS_CONFIG['results_dir'], result_file)\n",
    "        if os.path.exists(result_path):\n",
    "            shutil.copy2(result_path, results_export_dir)\n",
    "    \n",
    "    # Copy model if available\n",
    "    if os.path.exists(ANALYSIS_CONFIG['best_model_path']):\n",
    "        shutil.copy2(ANALYSIS_CONFIG['best_model_path'], drive_export_dir)\n",
    "    \n",
    "    print(f\"Results exported to Google Drive: {drive_export_dir}\")\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    summary = {\n",
    "        'analysis_completed': datetime.now().isoformat(),\n",
    "        'model_performance': test_results if test_results else {},\n",
    "        'cross_validation': cv_results if cv_results else {},\n",
    "        'training_history': {\n",
    "            'epochs_trained': len(training_history['loss']) if training_history else 0,\n",
    "            'final_training_loss': training_history['loss'][-1] if training_history and 'loss' in training_history else None,\n",
    "            'final_training_accuracy': training_history['accuracy'][-1] if training_history and 'accuracy' in training_history else None\n",
    "        },\n",
    "        'files_generated': {\n",
    "            'visualizations': len(os.listdir(viz_export_dir)) if os.path.exists(viz_export_dir) else 0,\n",
    "            'results': len(os.listdir(results_export_dir)) if os.path.exists(results_export_dir) else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_path = os.path.join(drive_export_dir, 'analysis_summary.json')\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Analysis summary saved to {summary_path}\")\n",
    "else:\n",
    "    print(\"\\nResults saved locally. Google Drive export skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SCHIZOPHRENIA DETECTION MODEL ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n🔧 Model Information:\")\n",
    "print(f\"  - Architecture: SSPNet 3D CNN\")\n",
    "print(f\"  - Input Shape: {default_config.model.input_shape}\")\n",
    "print(f\"  - Total Parameters: {model.count_params():,}\")\n",
    "print(f\"  - Number of Classes: {default_config.model.num_classes}\")\n",
    "\n",
    "if training_history:\n",
    "    print(f\"\\n📊 Training Results:\")\n",
    "    print(f\"  - Epochs Trained: {len(training_history['loss'])}\")\n",
    "    print(f\"  - Final Training Loss: {training_history['loss'][-1]:.4f}\")\n",
    "    print(f\"  - Final Training Accuracy: {training_history['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    if 'val_accuracy' in training_history:\n",
    "        print(f\"  - Final Validation Accuracy: {training_history['val_accuracy'][-1]:.4f}\")\n",
    "        \n",
    "        # Calculate overfitting\n",
    "        overfitting = training_history['accuracy'][-1] - training_history['val_accuracy'][-1]\n",
    "        print(f\"  - Overfitting Gap: {overfitting:.4f}\")\n",
    "\n",
    "if test_results:\n",
    "    print(f\"\\n🧪 Test Set Performance:\")\n",
    "    print(f\"  - Accuracy: {test_results['accuracy']:.4f}\")\n",
    "    print(f\"  - Precision: {test_results['precision']:.4f}\")\n",
    "    print(f\"  - Recall: {test_results['recall']:.4f}\")\n",
    "    print(f\"  - F1 Score: {test_results['f1_score']:.4f}\")\n",
    "    print(f\"  - AUC: {test_results['auc']:.4f}\")\n",
    "    \n",
    "    # Performance assessment\n",
    "    accuracy = test_results['accuracy']\n",
    "    if accuracy > 0.8:\n",
    "        assessment = \"Excellent Performance (>80%)\"\n",
    "        emoji = \"🟢\"\n",
    "    elif accuracy > 0.7:\n",
    "        assessment = \"Good Performance (>70%)\"\n",
    "        emoji = \"🟡\"\n",
    "    else:\n",
    "        assessment = \"Needs Improvement (<70%)\"\n",
    "        emoji = \"🔴\"\n",
    "    \n",
    "    print(f\"  - Assessment: {emoji} {assessment}\")\n",
    "\n",
    "if cv_results:\n",
    "    print(f\"\\n🔄 Cross-Validation Results:\")\n",
    "    for metric, value in cv_results['mean_metrics'].items():\n",
    "        std = cv_results['std_metrics'][metric]\n",
    "        print(f\"  - {metric.replace('_', ' ').title()}: {value:.4f} ± {std:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 Generated Visualizations:\")\n",
    "if os.path.exists(default_config.visualization.output_dir):\n",
    "    viz_files = [f for f in os.listdir(default_config.visualization.output_dir) if f.endswith(('.png', '.html', '.pdf'))]\n",
    "    for viz_file in viz_files:\n",
    "        print(f\"  - {viz_file}\")\n",
    "\n",
    "print(f\"\\n💾 Output Directories:\")\n",
    "print(f\"  - Results: {ANALYSIS_CONFIG['results_dir']}\")\n",
    "print(f\"  - Visualizations: {default_config.visualization.output_dir}\")\n",
    "\n",
    "if IN_COLAB and ANALYSIS_CONFIG['save_to_drive']:\n",
    "    print(f\"  - Google Drive Export: {drive_export_dir}\")\n",
    "\n",
    "print(f\"\\n🎯 Key Findings:\")\n",
    "if test_results:\n",
    "    print(f\"  - Model achieves {test_results['accuracy']:.1%} accuracy on test set\")\n",
    "    print(f\"  - AUC of {test_results['auc']:.3f} indicates good discriminative ability\")\n",
    "    \n",
    "    if 'confusion_matrix' in test_results:\n",
    "        cm = np.array(test_results['confusion_matrix'])\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        print(f\"  - Sensitivity: {sensitivity:.3f}, Specificity: {specificity:.3f}\")\n",
    "\n",
    "print(f\"\\n📝 Recommendations:\")\n",
    "if test_results and test_results['accuracy'] > 0.8:\n",
    "    print(f\"  - Model performance is excellent, ready for clinical deployment\")\n",
    "    print(f\"  - Consider external validation on independent datasets\")\n",
    "elif test_results and test_results['accuracy'] > 0.7:\n",
    "    print(f\"  - Model shows good performance, consider hyperparameter tuning\")\n",
    "    print(f\"  - Increase training data size for better generalization\")\n",
    "else:\n",
    "    print(f\"  - Model needs improvement, consider architecture modifications\")\n",
    "    print(f\"  - Implement data augmentation and regularization techniques\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory before ending\n",
    "print(\"\\n🧹 Cleaning up memory...\")\n",
    "clear_memory()\n",
    "check_memory_usage()\n",
    "\n",
    "print(\"\\n✅ Results analysis notebook completed successfully!\")\n",
    "print(\"\\nAll visualizations and results have been generated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}